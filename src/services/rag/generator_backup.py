"""
Generator
Tek sorumluluk: LLM ile cevap √ºretmek.
"""
import logging

import google.generativeai as genai

logger = logging.getLogger(__name__)


class Generator:
    """
    LLM cevap √ºretici.
    
    Liskov Substitution i√ßin:
    - Aynƒ± interface'i implemente eden OpenAIGenerator yazƒ±labilir
    - Pipeline hangi generator gelirse onu kullanƒ±r
    """
    
    SYSTEM_PROMPT = """Sen CineMind AI, bir sinema uzmanƒ± asistansƒ±n.

KURALLAR:
1. Sadece verilen kaynaklardaki bilgileri kullan
2. Bilmiyorsan "Bu konuda bilgim yok" de
3. Spoiler i√ßeren cevaplarda uyar
4. T√ºrk√ße yanƒ±t ver
5. Kaynaklardan alƒ±ntƒ± yaparken belirt (√∂rn: "Senaryoya g√∂re...")
"""
    
    def __init__(self, api_key: str, model: str = "models/gemini-2.5-flash"):
        genai.configure(api_key=api_key)
        self._model = genai.GenerativeModel(model)
        self._model_name = model
        
        logger.info(f"ü§ñ Generator ba≈ülatƒ±ldƒ±: {model}")
    
    def generate(self, query: str, context: str) -> str:
        """
        Context ve sorgudan cevap √ºret.
        
        Args:
            query: Kullanƒ±cƒ± sorusu
            context: Formatlanmƒ±≈ü kaynak bilgisi
            
        Returns:
            LLM cevabƒ±
        """
        prompt = self._build_prompt(query, context)
        
        try:
            response = self._model.generate_content(prompt)
            answer = response.text
            
            logger.info(f"‚úÖ Cevap √ºretildi ({len(answer)} karakter)")
            return answer
            
        except Exception as e:
            logger.error(f"‚ùå LLM hatasƒ±: {e}")
            return f"√úzg√ºn√ºm, ≈üu anda cevap √ºretemiyorum. Hata: {type(e).__name__}"
    
    def _build_prompt(self, query: str, context: str) -> str:
        """Final prompt'u olu≈ütur."""
        return f"""{self.SYSTEM_PROMPT}

KAYNAKLAR:
{context}

KULLANICI SORUSU:
{query}

CEVAP:"""